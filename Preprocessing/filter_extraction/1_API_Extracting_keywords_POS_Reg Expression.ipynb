{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmcFQgU4s5F3"
   },
   "source": [
    "**HAVING keyphrasetransformer WITH REGULAR EXTRACTION**\n",
    "\n",
    "For 'First API' extracting following - query, startDate, endDate, Range, typeOfWork,UNSPSC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAjV3-9Cs-Qx"
   },
   "source": [
    "***Installing required libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDy9dF7ZrIag",
    "outputId": "bcbb683e-c67d-4c61-edde-3f35ff134a12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keyphrasetransformer in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from keyphrasetransformer) (4.33.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from keyphrasetransformer) (3.8.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from nltk->keyphrasetransformer) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from nltk->keyphrasetransformer) (4.65.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from nltk->keyphrasetransformer) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from nltk->keyphrasetransformer) (8.1.7)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (0.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (22.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from transformers->keyphrasetransformer) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->keyphrasetransformer) (4.7.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers->keyphrasetransformer) (2023.9.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from tqdm->nltk->keyphrasetransformer) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from requests->transformers->keyphrasetransformer) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from requests->transformers->keyphrasetransformer) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from requests->transformers->keyphrasetransformer) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from requests->transformers->keyphrasetransformer) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install keyphrasetransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\aanand\\studies\\cits5508\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYwSelsstNhE"
   },
   "source": [
    "***Function - \"extract_info\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ys1QRrDdqwR1"
   },
   "outputs": [],
   "source": [
    "#from keyphrasetransformer import KeyPhraseTransformer\n",
    "import re\n",
    "import dateutil.parser\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "def extract_info(doc):\n",
    "    # Load spaCy's English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    pos_tags_of_interest = ['PROPN', 'NOUN']\n",
    "    words_to_be_ignored = ['contract', 'tender', 'record', 'cost', 'price', \n",
    "                           'date', 'unspsc', 'code', \"good\", \"service\", \"work\" ]\n",
    "\n",
    "\n",
    "    # Initialize KeyPhraseTransformer\n",
    "    #kp = KeyPhraseTransformer()\n",
    "\n",
    "    # Initialize lists to store extracted dates and prices\n",
    "    dates = []\n",
    "    prices = []\n",
    "    codes = []\n",
    "\n",
    "    # Regular expressions for extracting date patterns (yyyy-mm-dd), codes, and prices\n",
    "    date_pattern = r'\\d{4}-\\d{2}-\\d{2}'\n",
    "    price_pattern = r'\\$\\d+(?:,\\d{3})*(?:\\.\\d{2})?'  # Matches currency values (e.g., $5,000.00 or $5000)\n",
    "    code_pattern = r'\\d{8}(?:,\\s*\\d{8})*'  # Matches one or more 8-digit codes separated by commas\n",
    "\n",
    "    # Extract dates using dateutil\n",
    "    date_strings = re.findall(date_pattern, doc)\n",
    "\n",
    "    # Initialize start_date and end_date as None\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    # Checking which should be start date and end date\n",
    "    if len(date_strings) >= 2:\n",
    "        date1 = dateutil.parser.parse(date_strings[0])\n",
    "        date2 = dateutil.parser.parse(date_strings[1])\n",
    "\n",
    "        if date1 < date2:\n",
    "            start_date = date1.strftime('%Y-%m-%d')\n",
    "            end_date = date2.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            start_date = date2.strftime('%Y-%m-%d')\n",
    "            end_date = date1.strftime('%Y-%m-%d')\n",
    "    elif len(date_strings) == 1:\n",
    "        # If only one date is given, assume it as the 'start_date'\n",
    "        start_date = dateutil.parser.parse(date_strings[0]).strftime('%Y-%m-%d')\n",
    "    elif len(date_strings) == 0:\n",
    "        # Check for years only\n",
    "        years = re.findall(r'\\b\\d{4}\\b', doc)\n",
    "        if len(years) > 0:\n",
    "            years = sorted(years)\n",
    "            date_object = datetime(int(years[0]), 1, 1)\n",
    "            start_date = date_object.strftime('%Y-%m-%d')\n",
    "            if len(years) >= 2:\n",
    "                date_object = datetime(int(years[1]), 1, 1)\n",
    "                end_date = date_object.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Extract UNSPSC codes using regular expressions\n",
    "    code_strings = re.findall(code_pattern, doc)\n",
    "    for code_str in code_strings:\n",
    "        codes.extend([code.strip('\"') for code in code_str.split(', ')])\n",
    "\n",
    "    # Extract prices using regular expressions\n",
    "    price_strings = re.findall(price_pattern, doc)\n",
    "    # Checking which is the highest and lowest range\n",
    "    if len(price_strings) >= 2:\n",
    "        price1 = int(price_strings[0].replace('$', '').replace(',', ''))\n",
    "        price2 = int(price_strings[1].replace('$', '').replace(',', ''))\n",
    "        # Determine the lowest and highest values\n",
    "        if price1 < price2:\n",
    "            start_price = price1\n",
    "            end_price = price2\n",
    "            prices = [start_price, end_price]\n",
    "        else:\n",
    "            start_price = price2\n",
    "            end_price = price1\n",
    "            prices = [start_price, end_price]\n",
    "    elif len(price_strings) == 1:\n",
    "        # If only one price is given, assume it as the 'start_price'\n",
    "        start_price = int(price_strings[0].replace('$', '').replace(',', ''))\n",
    "        prices = [start_price]\n",
    "\n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    doc_lower = doc.lower()\n",
    "\n",
    "    # Initialize type of work as an empty string\n",
    "    work_type = \"\"\n",
    "\n",
    "    # Check if \"Goods and Services\" appears in the user input\n",
    "    if \"goods and services\" in doc_lower:\n",
    "        work_type = \"Goods and Services\"\n",
    "\n",
    "    # Check if \"Works\" appears in the user input\n",
    "    elif \"works\" in doc_lower:\n",
    "        work_type = \"Works\"\n",
    "\n",
    "    # Pick only pure words\n",
    "    #Remove all ignore words\n",
    "    words_to_be_ignored = words_to_be_ignored + [word + 's' for word in words_to_be_ignored]\n",
    "    doc_lower_words = doc_lower.split()\n",
    "    \n",
    "    # Use list comprehension to keep only words not in the list of words to remove\n",
    "    filtered_doc_lower_words = [word for word in doc_lower_words if word not in words_to_be_ignored]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    doc_lower = ' '.join(filtered_doc_lower_words)    \n",
    "    words_with_only_alphabets = re.findall(r'\\b[a-zA-Z]+\\b', doc_lower)\n",
    "    doc = \" \".join(words_with_only_alphabets).title()\n",
    "    nlp_doc = nlp(doc)\n",
    "    keywords = \" \".join([token.text for token in nlp_doc if token.pos_ in pos_tags_of_interest ])\n",
    "\n",
    "    # Create a dictionary to store the extracted information\n",
    "    result_dict = {\n",
    "        \"query\": keywords,\n",
    "        \"startDate\": start_date,\n",
    "        \"endDate\": end_date,\n",
    "        \"Range\": prices,\n",
    "        \"typeOfWork\": work_type,\n",
    "        \"UNSPSCcode\": [int(code) for code in codes]\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a JSON string\n",
    "    result_json = json.dumps(result_dict, indent=4)\n",
    "\n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLFKjo6KtUze"
   },
   "source": [
    "***calling the function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4B54FSPQq85R",
    "outputId": "f52427ab-d052-4e5d-ab27-bf7aafbd8a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query\": \"Cctv\",\n",
      "    \"startDate\": \"2023-03-10\",\n",
      "    \"endDate\": \"2023-04-20\",\n",
      "    \"Range\": [\n",
      "        5000,\n",
      "        7000\n",
      "    ],\n",
      "    \"typeOfWork\": \"Goods and Services\",\n",
      "    \"UNSPSCcode\": [\n",
      "        80172000,\n",
      "        80101513,\n",
      "        41102614\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Input to Function\n",
    "doc = \"\"\"CCTV contracts having Goods and Services from 2023-03-10 to 2023-04-20 cost from $5000 to $7000 with UNSPSC Code 80172000, 80101513, 41102614\"\"\"\n",
    "\n",
    "#doc = \"\"\"CCTV and Computer Contracts from 2013 to 2007\"\"\"\n",
    "#doc = \"\"\"CCTV\"\"\"\n",
    "result = extract_info(doc)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
