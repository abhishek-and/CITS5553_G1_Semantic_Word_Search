{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab39d9c",
   "metadata": {},
   "source": [
    "Install libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f79fc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (3.12.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\anaconda\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in d:\\anaconda\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2021.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: sentence-transformers in d:\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.22.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (0.24.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (4.33.1)\n",
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (from sentence-transformers) (3.6.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.3)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2021.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\anaconda\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\lib\\site-packages (from torchvision->sentence-transformers) (8.4.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.22.4)\n",
      "Requirement already satisfied: torch in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (2.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rashi\\appdata\\roaming\\python\\python39\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: nltk in d:\\anaconda\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install nltk\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea9543b",
   "metadata": {},
   "source": [
    "Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb9adf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the necessary tokenizer data if not already downloaded\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6807741",
   "metadata": {},
   "source": [
    "Download and load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f90a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer\n",
    "model_name = \"bert-large-uncased\"  # You can use other BERT variants\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69db62b",
   "metadata": {},
   "source": [
    "Semantic word search function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f68430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_word_search(query_word, word_list, threshold=0.80):\n",
    "    # Encode the query word\n",
    "    query_input = tokenizer(query_word, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        query_output = model(**query_input)\n",
    "\n",
    "    # Extract the embeddings for the query word\n",
    "    query_embedding = query_output.last_hidden_state.mean(dim=1).squeeze()\n",
    "\n",
    "    # Encode the list of words\n",
    "    word_inputs = tokenizer(word_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        word_outputs = model(**word_inputs)\n",
    "\n",
    "    # Extract the embeddings for the list of words\n",
    "    word_embeddings = word_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Calculate cosine similarity between query and list of words\n",
    "    cosine_scores = torch.nn.functional.cosine_similarity(query_embedding, word_embeddings, dim=1)\n",
    "\n",
    "    # Convert cosine similarity scores to semantic scores and to normal (Python float) values\n",
    "    semantic_scores = [(score.item() + 1) / 2 for score in cosine_scores]  # Normalize to [0, 1] range and convert to float\n",
    "\n",
    "    # Create a list to store unique words and their corresponding semantic scores as pairs\n",
    "    unique_words_and_scores = []\n",
    "\n",
    "    # Iterate through the words and semantic scores\n",
    "    for word, score in zip(word_list, semantic_scores):\n",
    "        if word not in (pair[0] for pair in unique_words_and_scores):\n",
    "            if score > threshold:\n",
    "                unique_words_and_scores.append((word, score))\n",
    "    \n",
    "    # Separate the unique words and their scores into separate lists\n",
    "    unique_words, unique_scores = zip(*unique_words_and_scores)\n",
    "\n",
    "    return list(unique_words), list(unique_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c171d",
   "metadata": {},
   "source": [
    "# Change the json_input, file_name and file_path accordingly "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881786d",
   "metadata": {},
   "source": [
    "Extract data from JSON Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ab19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example JSON input\n",
    "json_input = '{\"reference_number\": \"CUAAFA201807032023AC\", \"query\": \"CCTV\"}'\n",
    "query_data = json.loads(json_input)  # Parse the JSON input\n",
    "query_word = query_data[\"query\"].lower()  # Extract the query word from the JSON\n",
    "reference_number = query_data[\"reference_number\"]  # Extract the reference number from the JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0cf8e5",
   "metadata": {},
   "source": [
    "Extract required  data from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96047b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Contracts_Dataset_With_Extract.csv\"  # Replace with the path to your CSV file\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Filter the DataFrame based on the reference_number\n",
    "filtered_data = df[df['Reference Number'] == reference_number]\n",
    "\n",
    "# Check if any data with the given reference_number exists\n",
    "if not filtered_data.empty:\n",
    "    # Define the columns to concatenate\n",
    "    columns_to_concat = ['Contract Title', 'Description', 'Tenders Content']  # Add your column names here\n",
    "\n",
    "    # Initialize an empty list to store non-empty column values\n",
    "    non_empty_values = []\n",
    "\n",
    "    # Iterate through the columns and append non-empty values to the list\n",
    "    for column in columns_to_concat:\n",
    "        column_values = filtered_data[column].values[0]\n",
    "        if not pd.isna(column_values) and column_values.strip():  # Check for non-null and non-empty values\n",
    "            non_empty_values.append(column_values)\n",
    "\n",
    "    if non_empty_values:\n",
    "        # Concatenate non-empty column values into a single string\n",
    "        text_data = \" \".join(non_empty_values)\n",
    "        text_data = text_data.lower()\n",
    "\n",
    "        # Tokenize the concatenated text data\n",
    "        tokenized_words = word_tokenize(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a45c4c",
   "metadata": {},
   "source": [
    "JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c95668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: arts, Semantic Score: 0.8242\n",
      "Word: culture, Semantic Score: 0.8428\n",
      "Word: current, Semantic Score: 0.8064\n",
      "Word: model, Semantic Score: 0.8067\n",
      "Word: stakeholder, Semantic Score: 0.8173\n",
      "{\n",
      "  \"words\": [\n",
      "    \"arts\",\n",
      "    \"culture\",\n",
      "    \"current\",\n",
      "    \"model\",\n",
      "    \"stakeholder\"\n",
      "  ],\n",
      "  \"semantic_scores\": [\n",
      "    0.8242495059967041,\n",
      "    0.8428006768226624,\n",
      "    0.8063924014568329,\n",
      "    0.8067045211791992,\n",
      "    0.8172733187675476\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "words_list, semantic_scores  = semantic_word_search(query_word, tokenized_words)\n",
    "\n",
    "# Print the semantic scores for each word along with the word itself\n",
    "for word, score in zip(words_list, semantic_scores):\n",
    "    print(f\"Word: {word}, Semantic Score: {score:.4f}\")\n",
    "\n",
    "# Create a JSON object with two arrays: words and semantic scores\n",
    "output_json = {\n",
    "    \"words\": words_list,\n",
    "    \"semantic_scores\": semantic_scores  # No need to convert tolist()\n",
    "}\n",
    "\n",
    "# Serialize the JSON object to a string\n",
    "output_json_str = json.dumps(output_json, indent=2)\n",
    "\n",
    "# Print the JSON string\n",
    "print(output_json_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
