{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa0e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyMuPDF\n",
    "# pip install python-docx\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm - On Terminal\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the dataset\n",
    "dataset = pd.read_excel('Contracts_Dataset.xlsx', dtype=str)\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Directory containing the .zip files\n",
    "download_directory = os.path.join(current_directory, 'Tender_Files_1')\n",
    "\n",
    "# Destination directory for extracted contents\n",
    "extract_directory = os.path.join(current_directory, 'Tender_Files_Extract_1')\n",
    "\n",
    "# POS Tags of interest\n",
    "pos_tags_of_interest = ['NOUN', 'VERB', 'ADJ', 'ADV']\n",
    "\n",
    "# NER Tags of interest\n",
    "ner_tags_of_interest = ['ORG', 'GPE', 'LOC', 'NORP', 'PRODUCT', 'EVENT', 'SCIENCE', 'ARTICLE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d07dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the destination directory exists\n",
    "if not os.path.exists(extract_directory):\n",
    "    os.makedirs(extract_directory)\n",
    "\n",
    "# Iterate through the files in the Tenders directory and unzip all of them\n",
    "for file_name in os.listdir(download_directory):\n",
    "    file_path = os.path.join(download_directory, file_name)\n",
    "    \n",
    "    # Check if the file has a .zip extension\n",
    "    if file_name.endswith('.zip'):\n",
    "        tender_reference_number = file_name.split('-')[0]\n",
    "        tender_extract_path = os.path.join(extract_directory, tender_reference_number)\n",
    "        try:\n",
    "            # Open the ZIP file\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                # Extract all contents to the destination directory\n",
    "                zip_ref.extractall(tender_extract_path)\n",
    "\n",
    "        except zipfile.BadZipFile as e:\n",
    "            print(f\"Error: {e} - {file_path} is not a valid ZIP file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        # File has been unzipped - Look for more zip files within extracted content and unzip them\n",
    "        for inner_file_name in os.listdir(tender_extract_path):\n",
    "            inner_file_path = os.path.join(tender_extract_path, inner_file_name)\n",
    "    \n",
    "            # Check if the file has a .zip extension\n",
    "            if inner_file_name.endswith('.zip'):\n",
    "                try:\n",
    "                    with zipfile.ZipFile(inner_file_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(tender_extract_path)\n",
    "                except zipfile.BadZipFile as e:\n",
    "                    print(f\"Error: {e} - {inner_file_path} is not a valid ZIP file.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8049b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import fitz\n",
    "import docx\n",
    "\n",
    "# Extract the textual content from all pdf and docx files\n",
    "tenders = [f for f in os.listdir(extract_directory) if os.path.isdir(os.path.join(extract_directory, f))]\n",
    "\n",
    "# Now, subfolders contains a list of subfolder names in the specified folder\n",
    "for tender_reference_number in tenders:\n",
    "    tender_file_path = os.path.join(extract_directory, tender_reference_number)\n",
    "    tender_summary_file_path = os.path.join(extract_directory, tender_reference_number + \".txt\")\n",
    "    \n",
    "    # Create an empty summary file first\n",
    "    with open(tender_summary_file_path, 'w') as file:\n",
    "        file.write('')\n",
    "    \n",
    "    for root, dirs, files in os.walk(tender_file_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            text_content = ''\n",
    "            \n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_document = fitz.open(file_path)\n",
    "\n",
    "                # Iterate through each page in the PDF\n",
    "                for page_num in range(pdf_document.page_count):\n",
    "                    page = pdf_document[page_num]\n",
    "                    text_content += page.get_text()\n",
    "\n",
    "                # Close the PDF document\n",
    "                pdf_document.close()\n",
    "\n",
    "            if file.endswith('.docx'):\n",
    "                doc = docx.Document(file_path)\n",
    "                for paragraph in doc.paragraphs:\n",
    "                    text_content += paragraph.text + '\\n'\n",
    "\n",
    "            with open(tender_summary_file_path, 'a', encoding='utf-8') as file_writer:\n",
    "                file_writer.write(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data (you may have already done this)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c02da17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize RAKE\n",
    "rake = Rake()\n",
    "\n",
    "trimmed_data = {'Reference Number': [], 'Contract Title': [], 'Description': [], \n",
    "                'UNSPSC Title': [], 'Supplier Name': [], 'Tenders Content': []}\n",
    "trimmed_df = pd.DataFrame(trimmed_data)\n",
    "rows_to_add = []\n",
    "\n",
    "# Loop through the DataFrame one row at a time\n",
    "for index, row in dataset.iterrows():\n",
    "    reference_number = row['Reference Number'].strip()\n",
    "    title = row['Contract Title'].strip()\n",
    "    description = row['Description'].strip()\n",
    "    soup = BeautifulSoup(description, 'lxml')\n",
    "    description_text = ''.join(soup.stripped_strings)\n",
    "    unspsc_title = row['UNSPSC Title'].strip()\n",
    "    supplier_name = str(row['Supplier Name']).strip()\n",
    "\n",
    "    \n",
    "    # Check if the extracted tender file exists for the reference number\n",
    "    tender_useful_content = ''\n",
    "    file_path = os.path.join(extract_directory, reference_number + \".txt\")\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as tender_file:\n",
    "            tender_contents = tender_file.read()\n",
    "\n",
    "            # Extract keywords using RAKE\n",
    "            rake.extract_keywords_from_text(tender_contents)  # You were missing this line\n",
    "            keywords = rake.get_ranked_phrases()\n",
    "\n",
    "            useful_tokens = [keyword for keyword in keywords]\n",
    "\n",
    "            tender_useful_content = \" \".join(useful_tokens).strip()\n",
    "        \n",
    "    \n",
    "    new_row = {\n",
    "        'Reference Number': reference_number,\n",
    "        'Contract Title': title,\n",
    "        'Description': description_text,\n",
    "        'UNSPSC Title': unspsc_title,\n",
    "        'Supplier Name': supplier_name,\n",
    "        'Tenders Content': tender_useful_content\n",
    "    }\n",
    "    rows_to_add.append(new_row)\n",
    "\n",
    "trimmed_df = pd.concat([trimmed_df, pd.DataFrame(rows_to_add)], ignore_index=True)\n",
    "trimmed_df.to_csv('trimmed_dataset_rake.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c84f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    36 24x7 forticare contract 12 24x7 forticare c...\n",
      "1    addendum 1 important request dfes211522 reques...\n",
      "2    addendum 1 important tender process tender pro...\n",
      "3    thursday 28 october 2021 clarification please ...\n",
      "4    ac 12 zone 1 filter theatre 11 room 2 610 h x ...\n",
      "Name: Tenders Content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "trimmed_df = pd.read_csv('trimmed_dataset_rake.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(trimmed_df['Tenders Content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd847f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
