{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "981060d534a9470a87ac323fc44cb577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/StableBeluga-7B\", use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"stabilityai/StableBeluga-7B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### System:\n",
      "You are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\n",
      "\n",
      "### User: Write me a poem please\n",
      "\n",
      "### Assistant:\n",
      " The melancholy moon gently lights the night,\n",
      "As a whispy breeze softly plays its flute,\n",
      "Creating the perfect atmosphere for a romantic night,\n",
      "Wrapped in warm, deep hugs, two souls in love entwine,\n",
      "Laughter spills forth, sharing hearts, memories divine.\n",
      "\n",
      "Unicorns and rainbows, imagination's delight,\n",
      "Embraced by love, their hearts in perfect delight,\n",
      "Harmoniously, they sway together, blissful and free,\n",
      "As the moon, stars, and heart-shaped chocolate, sigh in ecstasy.\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"### System:\\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don't do anything illegal.\\n\\n\"\n",
    "\n",
    "message = \"Write me a poem please\"\n",
    "prompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(\n",
    "    **inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"Laptop\",\n",
      "  \"start_date\": \"2017-01-01\",\n",
      "  \"end_date\": \"2019-12-31\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"### System:\\nInstruction:\n",
    "\n",
    "**Prompt:** Extract filters from the query and return the result in a structured JSON format. The input query will consist of two parts: a primary query and a filter. The filter will typically be in the format \"from [start_date] to [end_date]\" or some variation of it. Your task is to:\n",
    "\n",
    "1. Identify and extract the primary query, which represents the main search topic.\n",
    "2. Extract the start date and end date from the filter, if present.\n",
    "\n",
    "Please make sure the JSON format for the response adheres to the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"query\": \"Primary Query\",\n",
    "  \"start_date\": \"Start Date (YYYY-MM-DD)\",\n",
    "  \"end_date\": \"End Date (YYYY-MM-DD)\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Example Usage:** \"Extract filters from 'CCTV from 3 Oct 2017 to 3 Oct 2019'.\"\n",
    "\n",
    "**Response:** \n",
    "```\n",
    "{\n",
    "  \"query\": \"CCTV\",\n",
    "  \"start_date\": \"2017-10-03\",\n",
    "  \"end_date\": \"2019-10-03\"\n",
    "}\n",
    "```\n",
    "\\n\\n\"\"\"\n",
    "message = \"Get all documents that consists of Laptop from 2017 to 2019\"\n",
    "prompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\n",
    "prompt_tokens = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")[\"input_ids\"]\n",
    "start_index = prompt_tokens.shape[-1]\n",
    "\n",
    "output = model.generate(\n",
    "    prompt_tokens, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256\n",
    ")\n",
    "\n",
    "generation_output = output[0][start_index:]\n",
    "generation_text = tokenizer.decode(generation_output, skip_special_tokens=True)\n",
    "print(generation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Laptop', 'start_date': '2017-01-01', 'end_date': '2019-12-31'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(generation_text)\n",
    "\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
